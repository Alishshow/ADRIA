\documentclass[a4paper, 10pt, ]{article}

\input{misc/preamble.tex}

\def\oznacenieCasti{AR03 - LS2019}





\begin{document}





\fontsize{12pt}{22pt}\selectfont

\centerline{\textsf{Adaptívne riadenie} \hfill \textsf{\oznacenieCasti}}

\fontsize{18pt}{22pt}\selectfont





\begin{flushleft}
	\textbf{\textsf{O samonastavujúcom sa regulátore,\newline časť prvá}}
\end{flushleft}





\normalsize

\bigskip

\tableofcontents

\bigskip

\vspace{18pt}










% \section{Samonastavujúci sa regulátor}

\section{Konkrétny príklad}
\label{konkretPriklad}


Princíp samonastavujúceho sa regulátora (Self-Tuning Regulator, i.e. STR) spočíva v~tom, že v každej perióde vzorkovania sa identifikujú parametre modelu riadeného systému a následne, s~využitím identifikovaných parametrov modelu, sa pomocou určitej metódy vypočítajú parametre regulátora.

Hneď v prvej vete sme použili pojem perióda vzorkovania. Je teda zrejmé, že celý riadiaci systém bude pracovať v~diskrétnej časovej oblasti. Modelom riadeného systému bude ARX (Auto Regressive eXogenous) model. Štruktúra riadenia bude tzv.  „trojzložková“. Tromi zložkami sú polynómy $R$, $S$ a $T$. Tieto polynómy majú svoje koeficienty, ktoré sú zároveň parametrami riadiacej štruktúry, vlastne parametrami regulátora.

Skonkretizujme teraz tento všeobecný popis.





\subsection{Riadený systém}

Riadený systém nech predstavuje prenosová funkcia v~tvare
\begin{equation} \label{riadenySys}
	G(s) = \frac{0,15}{s^2 + 0,3s + 0,2}
\end{equation}
Ide o prenosovú funkciu druhého rádu, ktorá má v~čitateli polynóm nultého stupňa, teda nemá nuly.




\subsection{ARX model}

Nech modelom riadeného systému je ARX (AutoRegressive eXogenous) model. Vo~všeobecnosti ARX model má tvar
\begin{equation} \label{ARX}
	A(z^{-1})y(k) = B(z^{-1})u(k) + \xi(k)
\end{equation}
kde
\begin{equation}
	\begin{split}
		 B(z^{-1}) &= b_1z^{-1} + \ldots + b_{n_b}z^{-n_b} \\
		 A(z^{-1}) &= 1 + a_1z^{-1} + \ldots + a_{n_a}z^{-n_a}
	\end{split}
\end{equation}
a $\xi(k)$ predstavuje poruchy a šum. V ďalšom budeme uvažovať $\xi(k) = 0$. ARX model vyjadrený v tvare diferenčnej rovnice:
\begin{equation} \label{ARXdifRov}
	\begin{split}
		y(k) = & -a_1y(k-1) - \ldots - a_{n_a}y(k-n_a) +  b_1u(k-1) + \ldots + b_{n_b}u(k-n_b)
	\end{split}
\end{equation}

Nech modelom sústavy je diferenčná rovnica v tvare \eqref{ARXdifRov}, kde hodnoty $n_a = 2$ a~$n_b = 2$. Potom táto diferenčná rovnica má tvar
\begin{equation} \label{modelDifRov}
	y(k) =  - a_1y(k-1) - a_2y(k-2) + b_1u(k-1) + b_2u(k-2)
\end{equation}
V maticovom zápise:
\begin{equation} \label{modelMatic}
	y(k) = h^\naT \Theta
\end{equation}
kde $h^\naT = \begin{bmatrix} -y(k-1) & -y(k-2) & u(k-1) & u(k-2) \end{bmatrix}$  a~$\Theta = \begin{bmatrix} a_1 & a_2 & b_1 & b_2 \end{bmatrix}^\naT$.
Neznámymi parametrami modelu teda sú koeficienty $a_1$, $a_2$, $b_1$ a $b_2$.






\subsection{Štruktúra riadenia}

Štruktúra riadenia je zrejmá zo zápisu „trojzložkového“ zákona riadenia
\begin{equation} \label{zakonRiadenia}
	\begin{split}
		R(z^{-1})u(k) & = T(z^{-1})r(k) - S(z^{-1})y(k) \\
		u(k) & = \frac{T(z^{-1})}{R(z^{-1})}r(k) - \frac{S(z^{-1})}{R(z^{-1})}y(k)
	\end{split}
\end{equation}
kde $R$, $S$ a $T$ sú polynómy v tvare
\begin{equation}
	\begin{split}
		R(z^{-1}) & = 1 + r_1z^{-1} + r_2z^{-2} + \ldots +  r_{n_r}z^{-n_r}\\
		S(z^{-1}) & = s_0 + s_1z^{-1} + s_2z^{-2} + \ldots +  s_{n_s}z^{-n_s} \\
		T(z^{-1}) & = t_0 + t_1z^{-1} + t_2z^{-2} + \ldots +  t_{n_t}z^{-n_t}
	\end{split}
\end{equation}
Ako už bolo uvedené, koeficienty polynómov sú parametrami regulátora. Počet parametrov regulátora závisí od stupňa jednotlivých polynómov. Pre tento príklad sú stupne polynómov nasledovné: $n_r = 1$, $n_s = 1$ a $n_t = 0$. Potom počet parametrov regulátora je $n_r + n_s + 1 + n_t + 1$. Teda $1 + 1 + 1 + 0 + 1 = 4$.

Zákon riadenia je možné zapísať aj v tvare diferenčnej rovnice
\begin{equation}
	u(k) =  - r_1 u(k-1) - s_0 y(k) - s_1 y(k-1) + t_0 r(k)
\end{equation}




\subsection{Výpočet parametrov regulátora}

Metóda, pomocou ktorej sa v tomto prípade budú počítať parametre regulátora bude \emph{Pole placement}~--~Metóda rozmiestňovania pólov uzavretého regulačného obvodu (URO). Ako už názov naznačuje, metóda spočíva v~predpísaní rozmiestnenia pólov URO. Inými slovami, predpisujú sa korene charakteristického polynómu URO. Voľbou polohy pólov URO je možné zvoliť dynamické vlastnosti URO. Poloha pólov sa zadáva zvolením \emph{želaného polynómu}, ktorý má také korene, aké si želáme. Ak napr. žiadame, aby URO mal dynamiku druhého rádu, tak želaný polynóm bude 2.~stupňa.

Pripomeňme, že v tomto prípade ide o „diskrétny“ polynóm, pretože riadiaci systém pracuje v diskrétnej oblasti, t.j. model sústavy je „diskrétny“ a aj zákon riadenia je „diskrétny“.

Parametre regulátora sa vypočítajú z porovnania koeficientov pri rovnakých mocninách charakteristického polynómu URO a želaného polynómu. Charakteristický polynóm URO sa bude skladať z polynómov modelu sústavy a zo zložiek regulátora, čo sú polynómy vystupujúce v zákone riadenia. Koeficienty polynómov modelu sústavy považujeme za známe, pretože ich získame identifikáciou. Koeficienty polynómov zo zákona riadenia~--~parametre regulátora sú neznáme. Získame ich riešením rovnice, kde na jednej strane je charakteristický polynóm URO a na druhej strane je želaný polynóm. Takáto rovnica sa nazýva \emph{Diofantická rovnica}.








\section{Identifikácia parametrov lineárneho modelu}


Uvažujeme lineárny model v tvare \eqref{modelMatic}. Parametre modelu budeme určovať \emph{rekurzívnou metódou najmenších štvorcov}. Najskôr však odvodíme tzv. Off -- line odhad parametrov modelu.






\subsection{Metóda najmenších štvorcov}
Predpokladajme, že sme spravili $N$ experimentov -- meraní. Napr.: na vstup sústavy sme priviedli „vhodný“ signál a s nejakou periódou vzorkovania sme zaznamenávali hodnoty vstupného aj výstupného (zo sústavy) signálu. Teda máme nameraných $N$~hodnôt vstupu, ku ktorým prislúcha $N$ hodnôt výstupu.

V i-tom experimente sme namerali hodnotu výstupného signálu $y_i$. Nech model má odhadnuté „nejaké“ parametre. Potom ak privedieme na vstup modelu hodnotu $u_i$, čo je nameraná hodnota vstupného signálu prislúchajúca k $y_i$, na výstupe modelu bude odhad $\hat{y}_i$. Tento odhad bude vo všeobecnosti rozdielny od nameranej hodnoty. Namiesto „nejakých“ hodnôt parametrov modelu určme také, pre ktoré bude platiť, že suma štvorcov odchýlok vo všetkých nameraných bodoch bude minimálna.

Odchýlka v i-tom experimente je $e_i = y_i - \hat{y}_i$. Odhad výstupu v i-tom experimente je $\hat{y}_i = h_i^\naT \Theta$, čo je rovnaký zápis ako v \eqref{modelMatic}, len namiesto času $k$~je použitý index~$i$. Všetky odchýlky v $N$ meraniach sú:
\begin{equation}
	\begin{array}{*{4}c}
		e_1 & = &y_1 - h_1^\naT \Theta\\
		 & \vdots & \\
		e_N & = &y_N - h_N^\naT \Theta\\
	\end{array}
\end{equation}
čo možno zapísať aj takto:
\begin{equation}
	e = y -
	\left[{\begin{array}{*{20}c}
		h_1^\naT \\
		\vdots \\
		h_N^\naT
	\end{array}}\right]
	\Theta
\end{equation}
Vektor $ h_1^\naT$, za predpokladu, že začiatočný čas je $t_0 = 0$ a prvá vzorka vstupu a výstupu bola nameraná v čase $t(k = 1) = 1\cdot T_{vz}$ je $ h_1^\naT = \left[{ -y(0) \quad 0 \quad u(0) \quad 0 }\right]$. Analogicky $ h_5^\naT = { h(5)}^\naT =  \left[{ -y(4) \quad -y(3) \quad u(4) \quad u(3) }\right]$.

Pre lepšiu názornosť nech je nameraných 5 vzoriek (a tiež hodnoty $y(0)$, $u(0)$). Potom všetky odchýlky je možné zapísať takto:
\begin{equation}
	 e =  y -
		\left[{\begin{array}{*{20}c}
			-y(0) & 0 & u(0) & 0 \\
			-y(1) & -y(0) & u(1) & u(0) \\
			-y(2) & -y(1) & u(2) & u(1) \\
			-y(3) & -y(2) & u(3) & u(2) \\
			-y(4) & -y(3) & u(4) & u(3)
		\end{array}}\right]
		\Theta
\end{equation}
Všeobecný zápis
\begin{equation}
	 e = y - H \Theta
\end{equation}





Ako už bolo uvedené, vektor $\Theta$, čo je vektor parametrov modelu určíme tak, aby suma štvorcov odchýlok bola minimálna. Zaveďme účelovú funkciu v tvare
\begin{equation} \label{ucFcnBez}
	J(\Theta) = \frac{1}{2}  e^\naT e =  \frac{1}{2} {\left({ y -  H \Theta}\right)}^\naT {\left({ y -  H \Theta}\right)}
\end{equation}
Je potrebné nájsť extrém tejto účelovej funkcie, čo znamená derivovať ju podľa vektora parametrov modelu.
\begin{equation}
	\begin{split}
	J(\Theta) &= \frac{1}{2} \left( y -  H \Theta \right)^\naT   \left( y - H \Theta \right) = \frac{1}{2} \left( y^\naT y - y^\naT H \Theta - \Theta^\naT H^\naT y + \Theta^\naT H^\naT H \Theta \right)
\end{split}
\end{equation}
S využitím rovnosti
\begin{equation}
	\nabla_{x} \left( x^\naT a \right) = \nabla_{x} \left( a^\naT x \right) = a
\end{equation}
máme
\begin{align*}
	\nabla_\Theta \left( y^\naT y \right) &= 0 \\
	\nabla_\Theta \left( y^\naT H \Theta \right) &= \left( y^\naT H \right)^\naT = H^\naT y \\
	\nabla_\Theta \left( \Theta^\naT H^\naT y \right) &= H^\naT y \\
	\nabla_\Theta \left( \Theta^\naT H^\naT H \Theta \right) &= \left( H^\naT H \Theta + \left( \Theta^\naT H^\naT H \right)^\naT \right) = H^\naT H \Theta + H^\naT H \Theta
\end{align*}
teda
\begin{equation}
	\begin{split}
		\nabla_{\Theta} \left( J \right) &= \frac{1}{2} \left( - H^\naT y - H^\naT y + H^\naT H \Theta + H^\naT H \Theta \right)
		\\ & = \frac{1}{2} \left( - 2 H^\naT y + 2 H^\naT H \Theta \right)
		\\ & = - H^\naT y + H^\naT  H \Theta
	\end{split}
\end{equation}
V extréme je hodnota $\nabla_{\Theta} \left( J \right)$ nulová, teda
\begin{equation}
	\begin{split}
		0 &= - H^\naT y + H^\naT H \Theta \\
		H^\naT H \Theta &= H^\naT y
	\end{split}
\end{equation}
a nakoniec
\begin{equation} \label{GaussovVzorec}
	\Theta = \left( H^\naT H \right)^{-1} H^\naT y
\end{equation}
Rovnica \eqref{GaussovVzorec} sa nazýva Gaussov vzorec.

Keďže $\nabla_\Theta \nabla_\Theta \left(J \right) = \left(H^\naT H \right) = H^\naT H$ a $H^\naT  H =  R$ je kladne definitná, pretože $ H$~je nenulová a teda $ H^\naT  H$ je symetrická a kladne definitná, potom nájdený extrém je minimum. Matica $R$ sa nazýva informačná matica a $ P =  R^{-1}$ sa nazýva disperzná matica (alebo aj Kovariančná matica) s~$(n_a + n_b)$ riadkami a~$(n_a + n_b)$ stĺpcami.

Dosadením do Gaussovho vzorca získame Off--line odhad parametrov modelu.








\subsection{Rekurzívna metóda najmenších štvorcov}

Predpokladajme, že poznáme odhad parametrov modelu v predchádzajúcom kroku $(k-1)$ a chceme získať odhad parametrov modelu v aktuálnom kroku $k$. Ak poznáme odhad parametrov v kroku $(k-1)$, je zrejmé, že poznáme aj maticu $ P(k-1)$.

Pre lepšiu názornosť nech situácia je takáto: Aktuálny krok je $k = 2$. V kroku $(k-1)$, teda v kroku $k = 1$ bola matica $P(k-1)$. Ak prejdeme z kroku $(k-1)$ do kroku $k$, znemená to pridať nový riadok matice $H$. Novým riadkom je vektor $h^\naT(k)$. Pre krok $k$ môžme písať Gausov vzorec:
\begin{equation}
	\Theta(k) = \left(
		\begin{bmatrix} H^\naT(k-1) & h(k) \end{bmatrix}
		\begin{bmatrix} H(k-1) \\ h^\naT(k) \end{bmatrix}
		\right)^{-1}
		\cdot
		\begin{bmatrix} H^\naT(k-1) & h(k) \end{bmatrix}
		y(k)
\end{equation}
Odkiaľ matica $P(k)$ je
\begin{equation} \label{rekurP}
	\begin{split}
		P(k) & = \left(
			\begin{bmatrix} H^\naT(k-1) & h(k) \end{bmatrix}
			\begin{bmatrix} H(k-1) \\ h^\naT(k) \end{bmatrix}
			\right)^{-1}
		\\ & = \left( H^\naT(k-1) H(k-1) + h(k) h^\naT(k) \right)^{-1}
		\\ & = \left( P(k-1)^{-1} + h(k) h^\naT(k) \right)^{-1}
	\end{split}
\end{equation}

V rovnici \eqref{rekurP} sa vyskytuje inverzia matice, ktorej rozmer závisí od počtu parametrov modelu. Tento počet môže byť veľký, a inverzia takto veľkej matice môže byť nerealizovateľná. Použitie Woodburryho lemmy o~inverzií matíc rieši tento problém. Platí (v tomto texte nebudeme uvádzať dôkaz Woodburryho lemmy)
\begin{equation} \label{Woodburry}
	\left( A + B D \right)^{-1} = A^{-1} - A^{-1} B \left( D A^{-1} B + 1 \right)^{-1} D A^{-1}
\end{equation}

Použitím \eqref{Woodburry} prejde \eqref{rekurP} do tvaru
\begin{equation} \label{rekurzP}
	\begin{split}
		 P(k) & = P(k-1) - P(k-1) h(k) \left( 1 + h^\naT(k) P(k-1) h(k) \right)^{-1} \cdot h^\naT(k) P(k-1) \\
		 & = P(k-1) - Y(k) h^\naT(k) P(k-1)
	\end{split}
\end{equation}
kde
\begin{equation*}
	Y(k) = \frac{P(k-1) h(k)}{1 + h^\naT(k) P(k-1) h(k)}
\end{equation*}
je tzv. „zosilnenie“. Rovnica \eqref{rekurzP} je rekurzívnym vzťahom pre výpočet novej matice $P$ z predchádzajúcej matice $P$.

Odhad parametrov modelu v kroku $k$ je
\begin{equation}
	\Theta(k) = P(k) H^\naT(k) y(k) = P(k) \sum_{i = 1}^{k}{h(i) y_i}  =  P(k) \left( \sum_{i = 1}^{k-1}{h(i) y_i} + h(k) y(k) \right)
\end{equation}
Z \eqref{rekurP} je zrejmé že platí
\begin{subequations}
	\begin{align}
		P^{-1}(k) &= P(k-1) + h(k) h(k)^\naT \\
		P^{-1}(k-1) &= P^{-1}(k) - h(k) h^\naT(k)
	\end{align}
\end{subequations}
Potom
\begin{equation}
	\begin{split}
		\sum_{i = 1}^{k-1}{h^\naT(i) y_i} &= P^{-1}(k-1) \Theta(k-1) = \left( P^{-1}(k) - h(k) h^\naT(k) \right) \Theta(k-1) \\
		&=  P^{-1}(k) \Theta(k-1) - h(k) h^\naT(k) \Theta(k-1)
	\end{split}
\end{equation}
čo umožní písať odhad parametrov modelu v~kroku $k$~v~tvare
\begin{equation} \label{rekurTheta}
	\begin{split}
		\Theta(k)
		&=
		P(k)
		\left(
			P^{-1}(k)
			\Theta(k-1)
			-
 			h(k)
			h^\naT(k)
			\Theta(k-1)
			+
			h(k)
			y(k)
		\right)
		\\&=
		P(k)
		P^{-1}(k)
		\Theta(k-1)
		-
		P(k)
		h(k)
		h^\naT(k)
		\Theta(k-1)
		+
		P(k)
		h(k)
		y(k)
		\\&=
		I
		\Theta(k-1)
		+
		P(k)
		h(k)
		\left(
			y(k)
			-
			h^\naT(k)
			\Theta(k-1)
		\right)
		\\&=
		\Theta(k-1)
		+
		P(k)
		h(k)
		e(k)
	\end{split}
\end{equation}

Rovnicu \eqref{rekurTheta} môžeme priviesť aj do iného tvaru. Platí:
\begin{equation} \label{velkaRovnicaDole}
	\begin{split}
		\Theta(k) &= \Theta(k-1) + P(k) h(k) e(k) \\
		&= \Theta(k-1) + \left( P(k-1) - Y(k) h^\naT(k) P(k-1) \right) h(k) e(k) \\
		&= \Theta(k-1) + \left( P(k-1) h(k) - \frac {P(k-1) h(k) h^\naT(k) P(k-1) h(k)}{ 1 + h^\naT(k) P(k-1) h(k)} \right) e(k) \\
		&= \Theta(k-1)
			\\&+ \left( \frac{P(k-1) h(k) + P(k-1) h(k) h^\naT(k) P(k-1) h(k)}{1 + h^\naT(k) P(k-1) h(k)} \right.
			\\&- \left. \frac{P(k-1) h(k) h^\naT(k) P(k-1) h(k)}{1 + h^\naT(k) P(k-1) h(k)} \right) e(k) \\
		&= \Theta(k-1) + \left( \frac{P(k-1) h(k)}{1 + h^\naT(k) P(k-1) h(k)} \right) e(k)
			\\&+ \left( \frac{P(k-1) h(k) h^\naT(k) P(k-1) h(k)}{1 + h^\naT(k) P(k-1) h(k)} \right) e(k)
			\\&- \left( \frac{P(k-1) h(k) h^\naT(k) P(k-1) h(k)}{1 + h^\naT(k) P(k-1) h(k)} \right) e(k) \\
		&= \Theta(k-1) + \left( \frac{P(k-1) h(k)}{1 + h^\naT(k) P(k-1) h(k)} \right) e(k)
	\end{split}
\end{equation}
Potom
\begin{equation} \label{rekurzTheta}
	\Theta(k) = \Theta(k-1) + Y(k) e(k)
\end{equation}
Rovnica \eqref{rekurzTheta} je rekurzívnym vzťahom pre výpočet nových parametrov modelu z~predchádzajúcich hodnôt parametrov modelu.





\subsubsection{Súhrn}

Algoritmus rekurzívnej metódy najmenších štvorcov je zhrnutý v~Tabuľke~\ref{Algoritmus rekurzívnej MNŠ}.



\begin{table}[t]
	\centering

	\caption{Algoritmus rekurzívnej MNŠ}
	\label{Algoritmus rekurzívnej MNŠ}

	\begin{tabular*}{\textwidth}{  c l @{\extracolsep{\fill}} l }
		\toprule
		1. & Odchýlka (rezíduum) & $ \displaystyle e(k) = y(k) - h^\naT(k) \Theta(k-1) $ \\
		\midrule
		2. & Zosilnenie & $ \displaystyle Y(k) = \frac{P(k-1) h(k)}{1 + h^\naT(k) P(k-1) h(k)}$ \\
		\midrule
		3. & Kovariančná matica & $ \displaystyle P(k) = P(k-1) - Y(k) h^\naT(k) P(k-1)$ \\
		\midrule
		4. & Nový odhad parametrov & $ \displaystyle \Theta(k) = \Theta(k-1) + Y(k) e(k) $ \\
		\bottomrule
	\end{tabular*}
\end{table}






\subsubsection{Štart algoritmu}

Disperzná matica má začiatočný tvar
\begin{equation}
	P(0) = \alpha  I
\end{equation}
kde $I$ je jednotková matica rovnakého rozmeru ako $P$ a $\alpha$ je reálne číslo napríklad z~intervalu $\left\langle{ 10,\:10^6 }\right\rangle$.

Začiatočné hodnoty parametrov modelu môžu byť napríklad nulové, t.j. vektor $\Theta$ je nulový vektor príslušnej dĺžky:
\begin{equation}
	\Theta(0) = 0
\end{equation}
Nie je to však pravidlo a začiatočné hodnoty parametrov modelu môžu byť v princípe akékoľvek. Tieto hodnoty sa spravidla využívajú v ďalších výpočtoch a tak napríklad môže vzniknúť požiadavka, že niektorý z parametrov nemôže byť nulový (napr. pre delenie nulou) a podobne. Tiež môže existovať približný, hrubý odhad týchto hľadaných (identifikovaných) parametrov a tento je potom často výhodné použiť ako začiatočný.




\subsubsection{Modifikácia algoritmu - zabúdanie}


V účelovej funkcii \eqref{ucFcnBez} sa nepredpokladá žiadne váhovanie jednotlivých prvkov vektora odchýlok $e$. Všetky prvky majú rovnakú váhu. Z pohľadu rekurzívneho algoritmu to znamená, že najstaršie odchýlky majú rovnakú váhu ako najnovšie. Často však môže byť veľmi výhodné ak by novšie vzorky, teda novšie zistené odchýlky mali vyššiu váhu ako staršie. Inými slovami výhodné by bolo zabúdať na staršie odchýlky a uvažovať len tie novšie.

V účelovej funkcii \eqref{ucFcnBez} je možné uvažovať váhovaciu maticu $W$ nasledovne:
\begin{equation} \label{ucFcnZ2}
	J(\Theta) = \frac{1}{2} e^\naT W e
\end{equation}
kde matica
\begin{equation} \label{ucFcnZ}
	W =
	\begin{bmatrix}
		w_1 & 0 & \cdots & 0 \\
		 0 &  w_2 & \ddots & \vdots \\
		 \vdots & \ddots &  \ddots & 0 \\
		0 & \cdots & 0 & w_N
	\end{bmatrix}
\end{equation}
umožní realizovať váhovanie jednotlivých štvorcov odchýlok.

Ak zvolíme váhovacie koeficienty ako $w_i = \lambda ^{N-i}$ potom sa takého váhovanie nazýva exponenciálne zabúdanie. Číslo $\lambda$ sa volí z intervalu $\left( 0, 1\right\rangle$, pričom typické hodnoty sú $\lambda = 0,99$ či $\lambda = 0,95$. Potom je zrejmé, že najnovšia vzorka, najnovší štvorec odchýlky, má váhu (je prenásobený číslom) $w_N = \lambda ^{N-N} = 1$. Všetky ostatné váhovacie koeficienty majú nižšiu hodnotu (hodnota exponenciálne klesá ako sa zmenšuje poradové číslo $i$).

\bigskip

Ak aplikujeme ten istý postup pre získanie rekurzívneho algoritmu MNŠ ako v~prípade bez exponenciálneho zabúdania, tak verzia so zabúdaním vedie na algoritmus sumarizovaný v tabuľke \ref{Algoritmus rekurzívnej MNŠ_zabudanie}.


\begin{table}[t]
	\centering

	\caption{Algoritmus rekurzívnej MNŠ s exponenciálnym zabúdaním}
	\label{Algoritmus rekurzívnej MNŠ_zabudanie}

	\begin{tabular*}{\textwidth}{  c l @{\extracolsep{\fill}} l }
		\toprule
		1. & Odchýlka (rezíduum) & $\displaystyle e(k) = y(k) - h^\naT(k) \Theta(k-1)$ \\
		\midrule
		2. & Zosilnenie & $\displaystyle Y(k) = \frac{P(k-1) h(k)}{\lambda + h^\naT(k) P(k-1) h(k)}$ \\
		\midrule
		3. & Kovariančná matica & $\displaystyle P(k) = \frac{1}{\lambda}  \left( P(k-1) - Y(k) h^\naT(k) P(k-1) \right)$ \\
		\midrule
		4. & Nový odhad parametrov & $\displaystyle \Theta(k) = \Theta(k-1) + Y(k) e(k)$ \\
		\bottomrule
	\end{tabular*}
\end{table}



\section{Cvičenie druhé}
\label{cvicdruhe}

\begin{enumerate}

	\item Zrealizujte priebežnú identifikáciu parametrov ARX modelu tak ako to predpokladá konkrétny príklad v časti~\ref{konkretPriklad}. Vyskúšajte verziu bez exponenciálneho zabúdania a s exponenciálnym zabúdaním.

\end{enumerate}







\section*{Otázky a úlohy}
\addcontentsline{toc}{section}{Otázky a úlohy}


\begin{enumerate}

	\item Stručne vysvetlite princíp rekurzívnej metódy najmenších štvorcov.

	\item Napíšte Gaussov vzorec a podrobne vysvetlite jednotlivé prvky

	\item Vyjadrite ARX model v tvare diskrétnej prenosovej funkcie alebo v tvare diferenčnej rovnice.

	\item Odvoďte Gaussov vzorec a ukážte, že nájdený extrém je minimum.

	\item Aké (ktoré) prvky obsahuje signálny vektor pri priebežnej identifikácii metódou najmenších štvorcov? (Čo tvorí prvky signálneho vektora pri priebežnej identifikácii metódou najmenších štvorcov?)

\end{enumerate}






\end{document}
